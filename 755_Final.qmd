---
title: "Pysch 755 Final"
format: 
  html:
    toc: true 
    toc_depth: 4
    embed-resources: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---
# Project Overview
This project focuses on the importance of fraud detection with automotive insurance. 

# Set up 
## Set up Global Options and import packages
```{r}
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
library(tidyverse) 
library(tidymodels)
library(xfun, include.only = "cache_rds")
library(keras, exclude = "get_weights")
library(magrittr, exclude = c("set_names", "extract"))
library(themis)
library(datadictionary)
path_data <- "data"
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max=Inf)
rerun_setting <- FALSE

cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```
In addition, I set up some parallel processing to help speed up computational cost of running these neural networks. 
## Read in the Data
```{r}
data_all <- read_csv(here::here(path_data, 'insurance_claims.csv'), show_col_types = FALSE) |> 
  janitor::clean_names() |> glimpse() 

skimr::skim(data_all)
```
We read the data in and converted all the variables to snake case for better analysis. As well as looked at some basic summary statistics for the data. 

## Exploratory Data Analysis
```{r}
data_all  <- data_all |>
  mutate(across(where(is.character), ~replace(., . == "?", NA)),
         property_damage_yes = if_else(property_damage == 'NO', -.5, .5),
         police_report_available_yes = if_else(police_report_available == 'NO', -.5, .5)) |> 
  select(-property_damage, -police_report_available, -'c39')

glimpse(data_all)
```
I had to change all the missing variables in the dataset from '?' to 'NA' which will make analyses smoother. As well as making 'property_damage' and 'police_report_available' into dummy coded variables. This will also be more useful when fitting my recipes. Then removed the original variables so that the model did not try to run those variables instead of the new ones. There was an additional variables that was called 'c39', which had completely NA values in response to it, so it can removed it. 

# Creat a Data Dictionary
## Find all variable names
```{r}
variable_names <- colnames(data_all)
variable_names
```

## Define the variables
```{r}
variable_names <- c(
  "months_as_customer", "age", "policy_number", "policy_bind_date",
  "policy_state", "policy_csl", "policy_deductable", "policy_annual_premium",
  "umbrella_limit", "insured_zip", "insured_sex", "insured_education_level",
  "insured_occupation", "insured_hobbies", "insured_relationship", "capital_gains",
  "capital_loss", "incident_date", "incident_type", "collision_type",
  "incident_severity", "authorities_contacted", "incident_state", "incident_city",
  "incident_location", "incident_hour_of_the_day", "number_of_vehicles_involved", "property_damage_yes",
  "bodily_injuries", "witnesses", "police_report_available_yes", "total_claim_amount",
  "injury_claim", "property_claim", "vehicle_claim", "auto_make",
  "auto_model", "auto_year", "fraud_reported"
)
```
## Add descriptions
```{r}
variable_descriptions <- c(
  "Length of time (in months) the customer has been with the insurer",
  "Age of the primary insured person",
  "Unique identifier for the insurance policy",
  "Date the policy was bound (i.e., became active)",
  "State in which the policy was issued",
  "Combined single limit for liability coverage (e.g., '250/500')",
  "Deductible amount on the policy",
  "Annual premium charged for the policy",
  "Additional umbrella liability coverage limit",
  "ZIP code of the insured",
  "Sex of the insured (e.g., MALE, FEMALE)",
  "Highest education level attained by the insured",
  "Occupation of the insured individual",
  "Insured’s hobbies (e.g., reading, skiing)",
  "Relationship of the insured to the policyholder",
  "Declared capital gains on the account",
  "Declared capital loss on the account",
  "Date on which the incident occurred",
  "Type of incident (e.g., Collision, Theft, Hail)",
  "Type of collision (e.g., Rear-End, Side, Front)",
  "Severity level of the incident (e.g., Major, Minor)",
  "Which authority was contacted after the incident",
  "State where the incident occurred",
  "City where the incident occurred",
  "Location description of the incident",
  "Hour of day the incident occurred (0–23)",
  "Number of vehicles involved in the incident",
  "Whether there was property damage (YES, 0.5/NO, -0.5)",
  "Number of bodily injuries reported",
  "Number of witnesses to the incident",
  "Whether a police report was available (YES, 0.5/NO, -0.5)",
  "Total dollar amount claimed for the incident",
  "Dollar amount claimed for bodily injury",
  "Dollar amount claimed for property damage",
  "Dollar amount claimed for vehicle damage",
  "Make of the insured automobile (e.g., Toyota)",
  "Model of the insured automobile (e.g., Corolla)",
  "Year the insured automobile was manufactured",
  "Whether the claim was reported as fraud (Y/N)"
)
```

## Create into a table
```{r}
data_dictionary <- data.frame(
  `Variable_Name` = variable_names,
  `Variable_Description` = variable_descriptions,
  stringsAsFactors = FALSE
)

data_dictionary
```
This creates a data dictionary that we can reference when examining the variables. 

# Observe Correlations
```{r}
sub_data <- data_all |> 
  select(auto_year, auto_make, 
                    property_damage_yes, bodily_injuries, injury_claim, witnesses, fraud_reported)

GGally::ggpairs(sub_data)
```
We can see here that there are not many strong correlations with these variables. The strongest correlation that we have is for 'injury_claim' and 'property_damage_yes', with just .05. This will be useful when trying to make interactions in our recipes. 

# Split the data
```{r}
set.seed(123456)
splits_test <- data_all |> 
  initial_split(prop = 2/3, strata = "fraud_reported")

data_trn <- splits_test |> 
  analysis()

data_test <- splits_test |> 
  assessment()

split_val <- validation_split(data_trn, prop = c(3/4), strata = "fraud_reported")
```
We use a stratified split to ensure balanced representation of fraudulent and non-fraudulent cases in both training and testing sets. The 2/3 proportion for training provides sufficient data for model development while reserving adequate data for final evaluation. The validation split creates a further subdivision of the training data for hyperparameter tuning and model selection, helping prevent overfitting to the test set.

## Examine Distributions
```{r}
key_vars <- c("auto_make", "injury_claim", "property_claim", 
              "vehicle_claim", "bodily_injuries", "witnesses")

comparison_plots <- key_vars |> 
  map(\(var_name) {
    data_trn |>
      ggplot(aes(x = fraud_reported, y = !!sym(var_name), fill = fraud_reported)) +
      geom_boxplot(alpha = 0.7) +
      geom_jitter(width = 0.2, alpha = 0.3, size = 0.5) +
      labs(title = str_to_title(str_replace_all(var_name, "_", " ")),
           x = "Fraud Reported",
           y = var_name) +
      theme_minimal() +
      theme(legend.position = "none") +
      scale_fill_manual(values = c("N" = "blue", "Y" = "red"))
  })

comparison_plots
```
These plots help visualize the distributions of the variables and tell whether or not what certain patterns in the data persist. For example, witnesses being present may have a strong impact on fraud detection.

# Build Mode 1
## Recipe Creation
```{r}
rec_1 <- recipe(fraud_reported ~ ., data = data_trn) |>
  step_rm(incident_location, insured_zip, policy_bind_date, incident_date,
          policy_number, insured_hobbies) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_log(policy_annual_premium, total_claim_amount, injury_claim,
           property_claim, vehicle_claim, capital_gains, offset = 1) |> 
  step_other(all_nominal_predictors(), threshold = 0.05, other = "other") |>
  step_dummy(all_nominal_predictors()) |>
  step_interact(terms = ~ injury_claim:property_damage_yes) |>
  step_interact(terms = ~ police_report_available_yes:injury_claim) |> 
  step_smote(fraud_reported) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

feat_range_trn <- rec_1 |> 
  prep(data_trn) |> 
  bake(NULL)

```
This recipe implements many different step functions to allow for higher accuracy scores. We are regressing fraud_reported on all of the other variables. This is to allow our dataset to be large enough to run a model with hidden layers and not overfit to the training set. In addition, we needed to use 'step_smote()' because the data was heavily imbalanced to 'No' for 'fraud_reported'. By doing this, it will make our model less likely to always predict 'No' and balance the data better. 

```{r}
fit_seeds <- sample.int(10^5, size = 3)

fit_nnet_range <- cache_rds(
  expr = {
    mlp(
    hidden_units = 24,
    penalty = 0.003,
    epochs = 100
)  |> 
      set_mode("classification") |> 
      set_engine("keras", callbacks = callback_early_stopping(patience = 10),
                 seeds = fit_seeds, 
                 metrics = c("accuracy", "AUC"),
                 validation_split = 0.1) |> 
      fit_resamples(
        preprocessor = rec_1,         
        resamples = split_val,        
        metrics = metric_set(accuracy, roc_auc)
      )
  }, 
  dir = "cache/",
  file = "fit_nnet_range",
  rerun = rerun_setting
)

fit_nnet_range$.metrics
```
This model was selected with these hyperparameters, after trial and error to get the best score for the recipe created. Even still, the accuracy was only 73% and the roc was 75%. These are not that good of scores, so we can work to improve these recipes. 

# Build Model 2
## Recipe 2 Creation
```{r}

rec_2 <- recipe(fraud_reported ~ ., data = data_trn) |>
  step_rm(incident_location, insured_zip, policy_bind_date, incident_date,
          policy_number, insured_hobbies) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_log(policy_annual_premium, total_claim_amount, injury_claim,
           property_claim, vehicle_claim, capital_gains, offset = 1) |> 
  step_other(all_nominal_predictors(), threshold = 0.05, other = "other") |>
  step_dummy(all_nominal_predictors()) |>
  step_interact(terms = ~ all_numeric_predictors():all_numeric_predictors()) |>
  step_smote(fraud_reported) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

feat_range_trn <- rec_2 |> 
  prep(data_trn) |> 
  bake(NULL)
```
This recipe follows many of the same steps as the recipe in the first model, however, it is now looking for every interaction with every numeric predictor. This will help look for a better accuracy at the cost of computation speed. With the data being as imbalanced as it is, there are only a few other steps that could help improve this models accuracy.  

## Fit nnet_m2
```{r}
fit_seeds <- sample.int(10^5, size = 3)

nnet_m2 <- cache_rds(
  expr = {
    mlp(
    hidden_units = 19,
    penalty = 0.0054,
    epochs = 61
)  |> 
      set_mode("classification") |> 
      set_engine("keras", callbacks = callback_early_stopping(patience = 10),
                 seeds = fit_seeds, 
                 metrics = c("accuracy", "AUC"),
                 validation_split = 0.1) |> 
      fit_resamples(
        preprocessor = rec_2,         
        resamples = split_val,        
        metrics = metric_set(accuracy, roc_auc)
      )
  }, 
  dir = "cache/",
  file = "nn_m2",
  rerun = rerun_setting
)

nnet_m2$.metrics
```
After running these models, we was able to increase both accuracy and roc to 77% and 78%, which are a lot better, but still not the best. This can be improved upon with a larger dataset, as having these hidden layers may increase the noise that is overfit in the training set, not the validation set. 



